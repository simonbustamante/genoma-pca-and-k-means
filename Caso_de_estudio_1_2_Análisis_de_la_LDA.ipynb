{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/simonbustamante/genoma-pca-and-k-means/blob/master/Caso_de_estudio_1_2_An%C3%A1lisis_de_la_LDA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v_XmdRf_Qe3d"
      },
      "source": [
        "# Caso de estudio 1.2: Análisis de la LDA (*Latent Dirichlet Allocation*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pszTrc8RQe3d"
      },
      "source": [
        "Librerías a importar:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17gTCD0xQe3e"
      },
      "outputs": [],
      "source": [
        "import sys, re, time, string, random, csv, argparse\n",
        "import requests\n",
        "import numpy as n\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from scipy.special import psi\n",
        "\n",
        "#Librerías de web scraping\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "#Librerías de NLP\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import wordpunct_tokenize\n",
        "\n",
        "\n",
        "print('\\n¡Librerías importadas con éxito!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rtJVGQOBQe3h"
      },
      "source": [
        "# Generación de la base de datos (*Web Scraping*)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sxL1oYykQe3i"
      },
      "source": [
        "En primer lugar debemos obtener la lista de profesores del departamento de EECS, y el lab al que pertenecen. Para ello podemos usar librerías de _Web scraping_ como `BeautifulSoup`. Este tipo de librerías ofrecen funciones para explorar el código fuente de páginas web, y obtener información de su contenido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mvW0yp-mQe3i"
      },
      "outputs": [],
      "source": [
        "url_eecs_fac = 'https://www.eecs.mit.edu/role/faculty/'\n",
        "h = {\"user-agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/16.4 Safari/605.1.15\"}\n",
        "html_data = requests.get(url_eecs_fac, headers=h).text\n",
        "soup = BeautifulSoup(html_data, 'html.parser')\n",
        "\n",
        "#Guardando el nombre y el laboratorio en dos listas\n",
        "fac = []\n",
        "labs = []\n",
        "entries = soup.find_all('div', class_='people-entry')\n",
        "for entry in entries:\n",
        "    name = entry.find('h5').find('a').text.strip()\n",
        "    lab = entry.find('div', class_='people-research').find_all('a')\n",
        "    lab_names = [a.text for a in lab]\n",
        "    fac.append(name)\n",
        "    labs.append(lab_names)\n",
        "fac_dept = list(zip(fac, labs))\n",
        "fac_dept = [(tup[0], ', '.join(tup[1])) for tup in fac_dept if tup[1]]\n",
        "\n",
        "\n",
        "print('Número de profesores de EECS: {}'.format(len(fac_dept)))\n",
        "print(fac_dept[:5],'...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "39gvGO4wpP8A"
      },
      "outputs": [],
      "source": [
        "html_data = requests.get(url_eecs_fac).text\n",
        "html_data\n",
        "soup = BeautifulSoup(html_data, 'html.parser')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WX5QWgixQe3o"
      },
      "source": [
        "Definimos una función para obtener todos los artículos de _arXiv_ de un autor determinado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9H-UOKsMQe3o"
      },
      "outputs": [],
      "source": [
        "def get_articles_for_author(author):\n",
        "    base_url = 'https://arxiv.org/search/?query=%22{name}%22&searchtype=author&abstracts=show&order=-announced_date_first&size=200'\n",
        "    author_query_url = base_url.format(name= author.replace(' ','+'))\n",
        "    query_result = requests.get(author_query_url).text\n",
        "    soup = BeautifulSoup(query_result, 'html.parser')\n",
        "    articles = soup.find_all(class_ = 'arxiv-result')\n",
        "\n",
        "    ids = [el.find(class_ = 'list-title is-inline-block').find('a').text.strip('arXiv:') for el in articles]\n",
        "    ids = [el.split('/')[1] if el.find('/')>=0 else el for el in ids]\n",
        "\n",
        "    titles = [el.find(class_ = 'title is-5 mathjax').text.strip(' \\n') for el in articles]\n",
        "    authors = [[author.text for author in el.find(class_ = 'authors').find_all('a')] for el in articles]\n",
        "    abstracts = []\n",
        "    for el in articles:\n",
        "        try:\n",
        "            abstracts.append(el.find(class_ = 'abstract-full has-text-grey-dark mathjax').text[9:-16])\n",
        "        except:\n",
        "            abstracts.append(el.find(class_ = 'abstract-short has-text-grey-dark mathjax').text[9:-16])\n",
        "    urls = [el.find(class_ = 'list-title is-inline-block').find('a')['href'] for el in articles]\n",
        "\n",
        "    return ids, titles, authors, urls, abstracts\n",
        "\n",
        "author = fac_dept[23][0]\n",
        "print('Ejemplo de un artículo de {}: \\n'.format(author))\n",
        "ids, titles, authors, urls, abstracts = get_articles_for_author(author)\n",
        "i = 0\n",
        "print('arXiv ID: {} (url: {} )'.format(ids[i],urls[i]))\n",
        "print('Título: {}'.format(titles[i]))\n",
        "print('Autores: {}'.format(authors[i]))\n",
        "print('---\\n{}\\n---'.format(abstracts[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PvhQtDjCQe3s"
      },
      "source": [
        "Una vez tenemos dicha función definida y hemos comprobado que funciona, podemos iterar la lista de profesores para obtener los artículos de cada profesor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t8WoqigZQe3s"
      },
      "outputs": [],
      "source": [
        "ids = []\n",
        "titles = []\n",
        "authors = []\n",
        "labs = []\n",
        "EECS_facs = []\n",
        "urls = []\n",
        "abstracts = []\n",
        "print('Descargando artículos de cada profesor:')\n",
        "t0 = time.time()\n",
        "for i,fac in enumerate(fac_dept):\n",
        "    id_list, title_list, author_list, url_list, abstract_list  = get_articles_for_author(fac[0])\n",
        "    ids += id_list\n",
        "    titles += title_list\n",
        "    authors += author_list\n",
        "    urls += url_list\n",
        "    abstracts += abstract_list\n",
        "    labs += [fac[1]]*len(id_list)\n",
        "    EECS_facs += [fac[0]]*len(id_list)\n",
        "    if round(i/10) == i/10:\n",
        "        print('{}/{} autores'.format(i,len(fac_dept)))\n",
        "tf = time.time()\n",
        "print('{} artículos descargados en {:.2f}s'.format(len(ids),tf-t0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bopZxfTLQe3u"
      },
      "source": [
        "Podemos guardar toda la información en una tabla de `pandas`, la siguiente celda muestra cómo hacerlo:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hW2W9OAEQe3u"
      },
      "outputs": [],
      "source": [
        "df = pd.DataFrame({'id':ids,'title':titles,'EECS_prof':EECS_facs,'lab':labs,'authors':authors,'url':urls,'abstract':abstracts})\n",
        "df[['id','lab']] = df[['id','lab']].drop_duplicates()\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVSp-7U6Qe3x"
      },
      "source": [
        "## Obtención del vocabulario"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rvtv367GQe3x"
      },
      "source": [
        "Para poder utilizar las funciones proporcionadas en el guión del caso de estudio se necesita un archivo de vocabulario en formato `.csv`. Podemos obtenerlo del repositorio del estudio original de LDA SVI:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cd7uZr8zQe3y"
      },
      "outputs": [],
      "source": [
        "url = 'https://raw.githubusercontent.com/blei-lab/onlineldavb/master/dictnostops.txt'\n",
        "vocab_list = pd.Series(requests.get(url).text.split('\\n')[:-1])\n",
        "vocab = {}\n",
        "for index, word in enumerate(vocab_list):\n",
        "    vocab[word] = index\n",
        "vocab_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I4sLQrz0Qe30"
      },
      "source": [
        "# LDA-SVI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AlpT9pV7Qe30"
      },
      "source": [
        "## Funciones auxiliares"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dqheWHoIQe33"
      },
      "source": [
        "### Generación de atributos de cada texto (recuento de palabras)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qjN28qlFQe33"
      },
      "outputs": [],
      "source": [
        "def parseDocument(doc, vocab):\n",
        "    wordslist = list()\n",
        "    countslist = list()\n",
        "    doc = doc.lower()\n",
        "    tokens = wordpunct_tokenize(doc)\n",
        "\n",
        "    dictionary = dict()\n",
        "    for word in tokens:\n",
        "        if word in vocab:\n",
        "            wordtk = vocab[word]\n",
        "            if wordtk not in dictionary:\n",
        "                dictionary[wordtk] = 1\n",
        "            else:\n",
        "                dictionary[wordtk] += 1\n",
        "\n",
        "    wordslist.append(list(dictionary.keys()))\n",
        "    countslist.append(list(dictionary.values()))\n",
        "    return (wordslist[0], countslist[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5MuLE7qQe35"
      },
      "source": [
        "### Cálculo de distribuciones de probabilidad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sltn_TgzQe36"
      },
      "outputs": [],
      "source": [
        "def dirichlet_expectation(alpha):\n",
        "    '''\n",
        "    For a vector theta ~ Dir(alpha), computes E[log(theta)] given alpha.\n",
        "\n",
        "    Taken from https://github.com/blei-lab/onlineldavb/blob/master/onlineldavb.py\n",
        "    '''\n",
        "    if (len(alpha.shape) == 1):\n",
        "        return (psi(alpha) - psi(n.sum(alpha)))\n",
        "    return (psi(alpha) - psi(n.sum(alpha, 1))[:, n.newaxis])\n",
        "\n",
        "def beta_expectation(a, b, k):\n",
        "    mysum = psi(a + b)\n",
        "    Elog_a = psi(a) - mysum\n",
        "    Elog_b = psi(b) - mysum\n",
        "    Elog_beta = n.zeros(k)\n",
        "    Elog_beta[0] = Elog_a[0]\n",
        "    # print Elog_beta\n",
        "    for i in range(1, k):\n",
        "        Elog_beta[i] = Elog_a[i] + n.sum(Elog_b[0:i])\n",
        "        # print Elog_beta\n",
        "    # print Elog_beta\n",
        "    return Elog_beta\n",
        "\n",
        "def plottrace(x, Y, K, n, perp):\n",
        "    for i in range(K):\n",
        "        plt.plot(x, Y[i], label = \"Topic %i\" %(i+1))\n",
        "\n",
        "    plt.xlabel(\"Number of Iterations\")\n",
        "    plt.ylabel(\"Probability of Each topic\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Trace plot for topic probabilities\")\n",
        "    plt.savefig(\"temp/plot_%i_%i_%f.png\" %(K, n, perp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ygL8w3ePQe38"
      },
      "source": [
        "## Implementación de la LDA mediante SVI"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xQO6bGQTQe38"
      },
      "source": [
        "Obtenemos la implementación de LDA mediante SVI de las fuentes mencionadas en el guión:\n",
        "* https://github.com/qlai/stochasticLDA\n",
        "* https://github.com/blei-lab/onlineldavb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gjUOfgycQe38"
      },
      "outputs": [],
      "source": [
        "n.random.seed(10000001)\n",
        "meanchangethresh = 1e-3\n",
        "MAXITER = 10000\n",
        "\n",
        "class SVILDA():\n",
        "    \"\"\"\n",
        "        Arguments:\n",
        "        K: Number of topics\n",
        "        vocab: A set of words to recognize. When analyzing documents, any word\n",
        "           not in this set will be ignored.\n",
        "        D: Total number of documents in the population. For a fixed corpus,\n",
        "           this is the size of the corpus. In the truly online setting, this\n",
        "           can be an estimate of the maximum number of documents that\n",
        "           could ever be seen.\n",
        "        alpha: Hyperparameter for prior on weight vectors theta\n",
        "        eta: Hyperparameter for prior on topics beta\n",
        "        tau: A (positive) learning parameter that downweights early iterations\n",
        "        kappa: Learning rate: exponential decay rate---should be between\n",
        "             (0.5, 1.0] to guarantee asymptotic convergence.\n",
        "        Note that if you pass the same set of D documents in every time and\n",
        "        set kappa=0 this class can also be used to do batch VB.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, vocab, K, D, alpha, eta, tau, kappa, docs, iterations, parsed = False):\n",
        "        self._vocab = vocab\n",
        "        self._V = len(vocab)\n",
        "        self._K = K\n",
        "        self._D = D\n",
        "        self._alpha = alpha\n",
        "        self._eta = eta\n",
        "        self._tau = tau\n",
        "        self._kappa = kappa\n",
        "        self._lambda = 1* n.random.gamma(100., 1./100., (self._K, self._V))\n",
        "        self._Elogbeta = dirichlet_expectation(self._lambda)\n",
        "        self._expElogbeta = n.exp(self._Elogbeta)\n",
        "        self._docs = docs\n",
        "        self.ct = 0\n",
        "        self._iterations = iterations\n",
        "        self._parsed = parsed\n",
        "        self._trace_lambda = {}\n",
        "        for i in range(self._K):\n",
        "            self._trace_lambda[i] = [self.computeProbabilities()[i]]\n",
        "        self._x = [0]\n",
        "\n",
        "    def updateLocal(self, doc): #word_dn is an indicator variable with dimension V\n",
        "        (words, counts) = doc\n",
        "        newdoc = []\n",
        "        N_d = sum(counts)\n",
        "        phi_d = n.zeros((self._K, N_d))\n",
        "        gamma_d = n.random.gamma(100., 1./100., (self._K))\n",
        "        Elogtheta_d = dirichlet_expectation(gamma_d)\n",
        "        expElogtheta_d = n.exp(Elogtheta_d)\n",
        "        for i, item in enumerate(counts):\n",
        "            for j in range(item):\n",
        "                newdoc.append(words[i])\n",
        "        assert len(newdoc) == N_d, \"error\"\n",
        "\n",
        "        for i in range(self._iterations):\n",
        "            for m, word in enumerate(newdoc):\n",
        "                phi_d[:, m] = n.multiply(expElogtheta_d, self._expElogbeta[:, word]) + 1e-100\n",
        "                phi_d[:, m] = phi_d[:, m]/n.sum(phi_d[:, m])\n",
        "\n",
        "            gamma_new = self._alpha + n.sum(phi_d, axis = 1)\n",
        "            meanchange = n.mean(abs(gamma_d - gamma_new))\n",
        "            if (meanchange < meanchangethresh):\n",
        "                break\n",
        "\n",
        "            gamma_d = gamma_new\n",
        "            Elogtheta_d = dirichlet_expectation(gamma_d)\n",
        "            expElogtheta_d = n.exp(Elogtheta_d)\n",
        "\n",
        "        newdoc = n.asarray(newdoc)\n",
        "        return phi_d, newdoc, gamma_d\n",
        "\n",
        "    def updateGlobal(self, phi_d, doc):\n",
        "\n",
        "        lambda_d = n.zeros((self._K, self._V))\n",
        "\n",
        "        for k in range(self._K):\n",
        "            phi_dk = n.zeros(self._V)\n",
        "            for m, word in enumerate(doc):\n",
        "                phi_dk[word] += phi_d[k][m]\n",
        "            lambda_d[k] = self._eta + self._D * phi_dk\n",
        "        rho = (self.ct + self._tau) **(-self._kappa)\n",
        "        self._lambda = (1-rho) * self._lambda + rho * lambda_d\n",
        "        self._Elogbeta = dirichlet_expectation(self._lambda)\n",
        "        self._expElogbeta = n.exp(self._Elogbeta)\n",
        "\n",
        "        if self.ct % 10 == 9:\n",
        "            for i in range(self._K):\n",
        "                self._trace_lambda[i].append(self.computeProbabilities()[i])\n",
        "            self._x.append(self.ct)\n",
        "\n",
        "    def runSVI(self):\n",
        "        for i in tqdm(range(self._iterations)):\n",
        "            randint = random.randint(0, self._D-1)\n",
        "            if self._parsed == False:\n",
        "                doc = parseDocument(self._docs[randint],self._vocab)\n",
        "            phi_doc, newdoc, gamma_d = self.updateLocal(doc)\n",
        "            self.updateGlobal(phi_doc, newdoc)\n",
        "            self.ct += 1\n",
        "\n",
        "    def computeProbabilities(self):\n",
        "        prob_topics = n.sum(self._lambda, axis = 1)\n",
        "        prob_topics = prob_topics/n.sum(prob_topics)\n",
        "        return prob_topics\n",
        "\n",
        "    def getTopics(self, docs = None):\n",
        "        prob_topics = self.computeProbabilities()\n",
        "        prob_words = n.sum(self._lambda, axis = 0)\n",
        "\n",
        "        if docs == None:\n",
        "            docs = self._docs\n",
        "        results = n.zeros((len(docs), self._K))\n",
        "        for i, doc in enumerate(docs):\n",
        "            parseddoc = parseDocument(doc, self._vocab)\n",
        "\n",
        "            for j in range(self._K):\n",
        "                aux = [self._lambda[j][word]/prob_words[word] for word in parseddoc[0]]\n",
        "                doc_probability = [n.log(aux[k]) * parseddoc[1][k] for k in range(len(aux))]\n",
        "                results[i][j] = sum(doc_probability) + n.log(prob_topics[j])\n",
        "        finalresults = n.zeros(len(docs))\n",
        "        for k in range(len(docs)):\n",
        "            finalresults[k] = n.argmax(results[k])\n",
        "        return finalresults, prob_topics\n",
        "\n",
        "    def calcPerplexity(self, docs = None):\n",
        "        perplexity = 0.\n",
        "        doclen = 0.\n",
        "        if docs == None:\n",
        "            docs =  self._docs\n",
        "        for doc in docs:\n",
        "            parseddoc = parseDocument(doc, self._vocab)\n",
        "            _, newdoc, gamma_d = self.updateLocal(parseddoc)\n",
        "            approx_mixture = n.dot(gamma_d, self._lambda)\n",
        "            # print(n.shape(approx_mixture))\n",
        "            approx_mixture = approx_mixture / n.sum(approx_mixture)\n",
        "            log_doc_prob = 0.\n",
        "            for word in newdoc:\n",
        "                log_doc_prob += n.log(approx_mixture[word])\n",
        "            perplexity += log_doc_prob\n",
        "            doclen += len(newdoc)\n",
        "            # print(perplexity, doclen)\n",
        "        perplexity = n.exp( - perplexity / doclen)\n",
        "        print(perplexity)\n",
        "        return perplexity\n",
        "\n",
        "    def plotTopics(self, perp):\n",
        "        plottrace(self._x, self._trace_lambda, self._K, self._iterations, perp)\n",
        "\n",
        "def test(k, iterations):\n",
        "\n",
        "    docs = getalldocs(\"alldocs2.txt\")\n",
        "    vocab = getVocab(\"dictionary2.csv\")\n",
        "\n",
        "    testset = SVILDA(vocab = vocab, K = k, D = len(docs), alpha = 0.2,\n",
        "                     eta = 0.2, tau = 1024, kappa = 0.7, docs = docs,\n",
        "                     iterations= iterations)\n",
        "    testset.runSVI()\n",
        "    finallambda = testset._lambda\n",
        "\n",
        "    heldoutdocs = getalldocs(\"testdocs.txt\")\n",
        "    perplexity = testset.calcPerplexity(docs = heldoutdocs)\n",
        "\n",
        "    with open(\"temp/%i_%i_%f_results.csv\" %(k, iterations, perplexity), \"w+\") as f:\n",
        "        writer = csv.writer(f)\n",
        "        for i in range(k):\n",
        "            bestwords = sorted(range(len(finallambda[i])), key=lambda j:finallambda[i, j])\n",
        "            bestwords.reverse()\n",
        "            writer.writerow([i])\n",
        "            for j, word in enumerate(bestwords):\n",
        "                writer.writerow([word, vocab.keys()[vocab.values().index(word)]])\n",
        "                if j >= 15:\n",
        "                    break\n",
        "    topics, topic_probs = testset.getTopics()\n",
        "    testset.plotTopics(perplexity)\n",
        "\n",
        "    for kk in range(0, len(finallambda)):\n",
        "        lambdak = list(finallambda[kk, :])\n",
        "        lambdak = lambdak / sum(lambdak)\n",
        "        temp = zip(lambdak, range(0, len(lambdak)))\n",
        "        temp = sorted(temp, key = lambda x: x[0], reverse=True)\n",
        "        # print temp\n",
        "        print('topic %d:' % (kk))\n",
        "        # feel free to change the \"53\" here to whatever fits your screen nicely.\n",
        "        for i in range(0, 10):\n",
        "            print('%20s  \\t---\\t  %.4f' % (vocab.keys()[vocab.values().index(temp[i][1])], temp[i][0]))\n",
        "\n",
        "    with open(\"temp/%i_%i_%f_raw.txt\" %(k, iterations, perplexity), \"w+\") as f:\n",
        "        # f.write(finallambda)\n",
        "        for result in topics:\n",
        "            f.write(str(result) + \" \\n\")\n",
        "        f.write(str(topic_probs) + \" \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t5mt0fFfQe3_"
      },
      "source": [
        "# Resultados"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aJDd0FCVQe3_"
      },
      "source": [
        "Una vez definidas todas las funciones necesarias sólo queda ejecutar el análisis de LDA:\n",
        "\n",
        "(este análisis le llevará unos 25 minutos... ¡tenga paciencia!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "46G_vbTVQe4A"
      },
      "outputs": [],
      "source": [
        "mode = 'normal'\n",
        "K = 5\n",
        "alpha = 0.2\n",
        "eta = 0.2\n",
        "tau = 1024\n",
        "kappa = 0.7\n",
        "iterations = 100000\n",
        "\n",
        "if mode == \"test\":\n",
        "    test(K, iterations)\n",
        "if mode == \"normal\":\n",
        "    docs = df.abstract.to_list()\n",
        "    D = len(docs)\n",
        "    print('number of docs: {}'.format(D))\n",
        "    lda = SVILDA(vocab = vocab, K = K, D = D, alpha = alpha,\n",
        "                 eta = eta, tau = tau, kappa = kappa, docs = docs,\n",
        "                 iterations = iterations)\n",
        "    lda.runSVI()\n",
        "    lda"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBPMG-kXQe4C"
      },
      "source": [
        "Podemos observar las distribución de probabilidades de cada palabra del vocabulario con respecto a cada tema identificado en los textos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COTEJXh0Qe4D"
      },
      "outputs": [],
      "source": [
        "lambda_df = pd.DataFrame({'word':list(vocab.keys())})\n",
        "for i in range(K):\n",
        "    lambda_df['Topic {}'.format(i+1)] = lda._lambda[i,:]\n",
        "lambda_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qI0Gi1PWQe4F"
      },
      "source": [
        "Para entender cómo el algoritmo de LDA ha calculado la distribución de probabilidades de cada tema podemos observar las 10 palabras con mayor probabilidad por tema:"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lambda_df.dtypes"
      ],
      "metadata": {
        "id": "D-nnMWck64pd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCd7s7J3Qe4F"
      },
      "outputs": [],
      "source": [
        "finalresults, prob_topics = lda.getTopics()\n",
        "\n",
        "for column,prob in list(zip(lambda_df.columns[1:],prob_topics)):\n",
        "    print('{} probability: {:.2f}%'.format(column,prob*100))\n",
        "    print(lambda_df.nlargest(10, [column])[['word',column]],'\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzTBn0pxQe4H"
      },
      "source": [
        "Finalmente, también podemos observar cómo ha quedado la distribución de temas sobre el conjunto de los documentos descargados:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gi2XptSZQe4H"
      },
      "outputs": [],
      "source": [
        "labels = lambda_df.columns[1:]\n",
        "sizes = prob_topics*100\n",
        "explode = (0.1, 0.1, 0.1,0.1, 0.1)\n",
        "\n",
        "fig1, ax1 = plt.subplots()\n",
        "ax1.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',\n",
        "        shadow=True, startangle=90)\n",
        "ax1.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle.\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGEHnz4kQe4J"
      },
      "source": [
        "# Tarea adicional"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "agKNlRxGQe4K"
      },
      "source": [
        "Intentar reproducir el Gráfico 2 del guión del Caso de estudio 1.2, donde se analiza qué temas predominan en cada laboratorio."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Código"
      ],
      "metadata": {
        "id": "TPYAqjIg8wAo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "hide_input": false,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": true,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}